!pip install datasets
from datasets import load_dataset
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from nltk import sent_tokenize,word_tokenize
import numpy as np
import io
import json
import pandas as pd
import re
from tensorflow.keras import layers
import unicodedata


dataset = load_dataset("scientific_papers", "pubmed")

train_data=dataset['train'][:500]
test_data=dataset['test']

train_input=train_data['article']
train_target=train_data['abstract']



def preprocess_sentence(s):
  s = re.sub(r"([?.!,¿])", r" \1 ", s)
  s = re.sub(r'[" "]+', " ", s)
  s = s.strip()
  return s

train_preprocessed_input = [preprocess_sentence(s) for s in train_input]
train_preprocessed_target = [preprocess_sentence(s) for s in train_target]

train_preprocessed_input[:1]  


def tag_target_sentences(sentences):
  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)
  return list(tagged_sentences)

train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)  

source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')
source_tokenizer.fit_on_texts(train_preprocessed_input)
source_tokenizer.get_config()


source_vocab_size = len(source_tokenizer.word_index) + 1
print(source_vocab_size)

target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')
target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)
target_tokenizer.get_config()

target_vocab_size = len(target_tokenizer.word_index) + 1
print(target_vocab_size)

train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)


def generate_decoder_inputs_targets(sentences, tokenizer):
  seqs = tokenizer.texts_to_sequences(sentences)
  decoder_inputs = [s[:-1] for s in seqs]
  decoder_targets = [s[1:] for s in seqs]

  return decoder_inputs, decoder_targets

train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target,target_tokenizer)

max_encoding_len = len(max(train_encoder_inputs, key=len))
max_encoding_len

max_decoding_len = len(max(train_decoder_inputs, key=len))
max_decoding_len

padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')
padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')
padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')

target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[0]])


def process_dataset(dataset):


  input=dataset['article']
  output=dataset['abstract']

  preprocessed_input = [preprocess_sentence(s) for s in input]
  preprocessed_output = [preprocess_sentence(s) for s in output]


  tagged_preprocessed_output = tag_target_sentences(preprocessed_output)


  encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)


  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output,
                                                                    target_tokenizer)


  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')
  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')
  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')

  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets


padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(test_data)


embedding_dim = 128
hidden_dim = 256
default_dropout=0.2
batch_size = 32
epochs = 30

encoder_inputs = layers.Input(shape=[None], name='encoder_inputs')

encoder_embeddings = layers.Embedding(source_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='encoder_embeddings')


encoder_embedding_output = encoder_embeddings(encoder_inputs)

encoder_lstm = layers.LSTM(hidden_dim,
                           return_state=True,
                           dropout=default_dropout,
                           name='encoder_lstm')


encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)

encoder_states = (state_h, state_c)

decoder_inputs = layers.Input(shape=[None], name='decoder_inputs')


decoder_embeddings = layers.Embedding(target_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='decoder_embeddings')


decoder_embedding_output = decoder_embeddings(decoder_inputs)


decoder_lstm = layers.LSTM(hidden_dim,
                           return_sequences=True,
                           return_state=True,
                           dropout=default_dropout,
                           name='decoder_lstm')

decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)

decoder_dense = layers.Dense(target_vocab_size, activation='softmax', name='decoder_dense')
y_proba = decoder_dense(decoder_outputs)


model =tf.keras.Model([encoder_inputs,decoder_inputs],y_proba,name='model_encoder_training')

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',  metrics=['sparse_categorical_accuracy'])
model.summary()


filepath="C:/Users/AHMET CEBİR KEMBER/Desktop/Programlama/python/AI"


cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,
                                                 save_weights_only=True,
                                                 verbose=1)

es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)


history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_targets,
                     batch_size=batch_size,
                     epochs=epochs,
                     validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets))


model.save('model')

